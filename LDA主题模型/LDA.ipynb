{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA模型应用：希拉里邮件门"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "HillaryEmail = pd.read_csv('HillaryEmails.csv')\n",
    "emailBodyText = HillaryEmail[['Id', 'ExtractedBodyText']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6742 entries, 1 to 7944\n",
      "Data columns (total 2 columns):\n",
      "Id                   6742 non-null int64\n",
      "ExtractedBodyText    6742 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 158.0+ KB\n"
     ]
    }
   ],
   "source": [
    "emailBodyText.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emailBodyTextcleaning(text):\n",
    "    text = text.replace('\\n',\" \")\n",
    "    text = re.sub(r\"-\", \" \", text)\n",
    "    text = re.sub(r\"\\d+/\\d+/\\d+\", \"\", text)\n",
    "    text = re.sub(r\"[0-2]?[0-9]:[0-6][0-9]\", \"\", text)\n",
    "    text = re.sub(r\"[\\w]+@[\\.\\w]+\", \"\", text)\n",
    "    text = re.sub(r\"/[a-zA-Z]*[:\\//\\]*[A-Za-z0-9\\-_]+\\.+[A-Za-z0-9\\.\\/%&=\\?\\-_]+/i\", \"\", text)\n",
    "    pure_text = ''\n",
    "    for letter in text:\n",
    "        if letter.isalpha() or letter==' ':\n",
    "            pure_text += letter\n",
    "    text = ' '.join(word for word in pure_text.split() if len(word)>1)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BodyText = emailBodyText['ExtractedBodyText'].apply(lambda c: emailBodyTextcleaning(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Thursday March PM Latest How Syria is aiding Qaddafi and more Sid hrc memo syria aiding libya docx hrc memo syria aiding libya docx March For Hillary'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BodyText.head(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodytextlist = BodyText.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用gensim库构建模型，其要求语料库形式为：\n",
    "\n",
    "[[一，条，邮件，在，这里],[第，二，条，邮件，在，这里],[今天，天气，肿么，样],...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "停止词，在中英文中没有意义，出现概率可能还比较高，需要去除..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordlist = ['very', 'ourselves', 'am', 'doesn', 'through', 'me', 'against', 'up', 'just', 'her', 'ours', \n",
    "            'couldn', 'because', 'is', 'isn', 'it', 'only', 'in', 'such', 'too', 'mustn', 'under', 'their', \n",
    "            'if', 'to', 'my', 'himself', 'after', 'why', 'while', 'can', 'each', 'itself', 'his', 'all', 'once', \n",
    "            'herself', 'more', 'our', 'they', 'hasn', 'on', 'ma', 'them', 'its', 'where', 'did', 'll', 'you', \n",
    "            'didn', 'nor', 'as', 'now', 'before', 'those', 'yours', 'from', 'who', 'was', 'm', 'been', 'will', \n",
    "            'into', 'same', 'how', 'some', 'of', 'out', 'with', 's', 'being', 't', 'mightn', 'she', 'again', 'be', \n",
    "            'by', 'shan', 'have', 'yourselves', 'needn', 'and', 'are', 'o', 'these', 'further', 'most', 'yourself', \n",
    "            'having', 'aren', 'here', 'he', 'were', 'but', 'this', 'myself', 'own', 'we', 'so', 'i', 'does', 'both', \n",
    "            'when', 'between', 'd', 'had', 'the', 'y', 'has', 'down', 'off', 'than', 'haven', 'whom', 'wouldn', \n",
    "            'should', 've', 'over', 'themselves', 'few', 'then', 'hadn', 'what', 'until', 'won', 'no', 'about', \n",
    "            'any', 'that', 'for', 'shouldn', 'don', 'do', 'there', 'doing', 'an', 'or', 'ain', 'hers', 'wasn', \n",
    "            'weren', 'above', 'a', 'at', 'your', 'theirs', 'below', 'other', 'not', 're', 'him', 'during', 'which']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人工分词：\n",
    "\n",
    "英文句式中都是包含空格的，直接空格分词就行；而对于中文稍微麻烦，可以使用CoreNLP、HaNLP，或结巴分词来处理...\n",
    "\n",
    "分词的意义在于将句子转换为有意义的小单元，让计算机来识别..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in bodytext.lower().split() if word not in stopwordlist] for bodytext in bodytextlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thursday',\n",
       " 'march',\n",
       " 'pm',\n",
       " 'latest',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'qaddafi',\n",
       " 'sid',\n",
       " 'hrc',\n",
       " 'memo',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'libya',\n",
       " 'docx',\n",
       " 'hrc',\n",
       " 'memo',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'libya',\n",
       " 'docx',\n",
       " 'march',\n",
       " 'hillary']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立语料库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "语料库建立会对每个词加上index，并会统计每个词的词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(51, 1), (505, 1), (506, 1), (507, 1), (508, 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出现次数为top5的单词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.025*\"call\" + 0.015*\"tomorrow\" + 0.011*\"talk\" + 0.010*\"im\" + 0.009*\"also\"'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topic(10, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印所有主题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"pm\" + 0.013*\"state\" + 0.011*\"fw\" + 0.010*\"cheryl\" + 0.008*\"mills\"'),\n",
       " (1,\n",
       "  '0.009*\"think\" + 0.006*\"would\" + 0.006*\"germany\" + 0.005*\"tomorrow\" + 0.005*\"one\"'),\n",
       " (2,\n",
       "  '0.011*\"know\" + 0.009*\"work\" + 0.007*\"good\" + 0.005*\"women\" + 0.005*\"holbrooke\"'),\n",
       " (3, '0.014*\"call\" + 0.012*\"add\" + 0.011*\"pls\" + 0.008*\"mod\" + 0.007*\"would\"'),\n",
       " (4,\n",
       "  '0.007*\"would\" + 0.006*\"government\" + 0.006*\"us\" + 0.005*\"party\" + 0.005*\"security\"'),\n",
       " (5,\n",
       "  '0.014*\"qddr\" + 0.005*\"arizona\" + 0.004*\"fco\" + 0.004*\"got\" + 0.004*\"called\"'),\n",
       " (6, '0.008*\"would\" + 0.007*\"mr\" + 0.005*\"said\" + 0.005*\"one\" + 0.005*\"us\"'),\n",
       " (7,\n",
       "  '0.029*\"fyi\" + 0.013*\"boehner\" + 0.012*\"doc\" + 0.008*\"house\" + 0.007*\"us\"'),\n",
       " (8,\n",
       "  '0.006*\"us\" + 0.005*\"new\" + 0.005*\"obama\" + 0.005*\"policy\" + 0.004*\"said\"'),\n",
       " (9,\n",
       "  '0.007*\"get\" + 0.006*\"didnt\" + 0.005*\"madame\" + 0.004*\"roger\" + 0.004*\"brother\"'),\n",
       " (10,\n",
       "  '0.025*\"call\" + 0.015*\"tomorrow\" + 0.011*\"talk\" + 0.010*\"im\" + 0.009*\"also\"'),\n",
       " (11,\n",
       "  '0.012*\"fyi\" + 0.009*\"us\" + 0.005*\"would\" + 0.005*\"un\" + 0.005*\"prince\"'),\n",
       " (12,\n",
       "  '0.008*\"new\" + 0.006*\"us\" + 0.005*\"senate\" + 0.005*\"united\" + 0.005*\"american\"'),\n",
       " (13,\n",
       "  '0.015*\"bloomberg\" + 0.008*\"woodward\" + 0.007*\"state\" + 0.007*\"thank\" + 0.006*\"back\"'),\n",
       " (14,\n",
       "  '0.019*\"israel\" + 0.015*\"israeli\" + 0.011*\"settlements\" + 0.009*\"netanyahu\" + 0.008*\"settlement\"'),\n",
       " (15,\n",
       "  '0.018*\"yes\" + 0.010*\"email\" + 0.007*\"copy\" + 0.007*\"get\" + 0.007*\"ill\"'),\n",
       " (16,\n",
       "  '0.023*\"print\" + 0.018*\"pls\" + 0.010*\"party\" + 0.007*\"pis\" + 0.006*\"labour\"'),\n",
       " (17,\n",
       "  '0.035*\"ok\" + 0.023*\"thx\" + 0.010*\"discuss\" + 0.010*\"pls\" + 0.007*\"part\"'),\n",
       " (18,\n",
       "  '0.084*\"pm\" + 0.036*\"office\" + 0.032*\"secretarys\" + 0.023*\"meeting\" + 0.022*\"room\"'),\n",
       " (19,\n",
       "  '0.021*\"see\" + 0.014*\"hikers\" + 0.009*\"happy\" + 0.008*\"like\" + 0.007*\"thanks\"')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_topics=20, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "希拉里twitter数据：\n",
    "\n",
    "```\n",
    "To all the little girls watching...never doubt that you are valuable and powerful & deserving of every chance & opportunity in the world.\n",
    "\n",
    "I was greeted by this heartwarming display on the corner of my street today. Thank you to all of you who did this. Happy Thanksgiving. -H\n",
    "\n",
    "Hoping everyone has a safe & Happy Thanksgiving today, & quality time with family & friends. -H\n",
    "\n",
    "Scripture tells us: Let us not grow weary in doing good, for in due season, we shall reap, if we do not lose heart.\n",
    "\n",
    "Let us have faith in each other. Let us not grow weary. Let us not lose heart. For there are more seasons to come and...more work to do\n",
    "\n",
    "We have still have not shattered that highest and hardest glass ceiling. But some day, someone will\n",
    "\n",
    "To Barack and Michelle Obama, our country owes you an enormous debt of gratitude. We thank you for your graceful, determined leadership\n",
    "\n",
    "Our constitutional democracy demands our participation, not just every four years, but all the time\n",
    "\n",
    "You represent the best of America, and being your candidate has been one of the greatest honors of my life\n",
    "\n",
    "Last night I congratulated Donald Trump and offered to work with him on behalf of our country\n",
    "\n",
    "Already voted? That's great! Now help Hillary win by signing up to make calls now\n",
    "\n",
    "It's Election Day! Millions of Americans have cast their votes for Hillary—join them and confirm where you vote\n",
    "\n",
    "We don’t want to shrink the vision of this country. We want to keep expanding it\n",
    "\n",
    "We have a chance to elect a 45th president who will build on our progress, who will finish the job\n",
    "\n",
    "I love our country, and I believe in our people, and I will never, ever quit on you. No matter what\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1条twitter最有可能属于topic0，属于该topic的概率为：0.905\n",
      "第2条twitter最有可能属于topic0，属于该topic的概率为：0.839\n",
      "第3条twitter最有可能属于topic7，属于该topic的概率为：0.502\n",
      "第4条twitter最有可能属于topic7，属于该topic的概率为：0.791\n",
      "第5条twitter最有可能属于topic7，属于该topic的概率为：0.775\n",
      "第6条twitter最有可能属于topic7，属于该topic的概率为：0.927\n",
      "第7条twitter最有可能属于topic7，属于该topic的概率为：0.754\n",
      "第8条twitter最有可能属于topic0，属于该topic的概率为：0.336\n",
      "第9条twitter最有可能属于topic4，属于该topic的概率为：0.792\n",
      "第10条twitter最有可能属于topic4，属于该topic的概率为：0.914\n",
      "第11条twitter最有可能属于topic12，属于该topic的概率为：0.427\n",
      "第12条twitter最有可能属于topic11，属于该topic的概率为：0.770\n",
      "第13条twitter最有可能属于topic11，属于该topic的概率为：0.770\n",
      "第14条twitter最有可能属于topic11，属于该topic的概率为：0.526\n"
     ]
    }
   ],
   "source": [
    "data_twitter = pd.read_excel('./HillaryTwitters.xlsx', names=['BodyText'])\n",
    "data_twitter.dropna(inplace=True)\n",
    "\n",
    "text = data_twitter['BodyText']\n",
    "text = text.apply(lambda s: emailBodyTextcleaning(s))\n",
    "textlist = text.values\n",
    "\n",
    "texts = [[word for word in doc.lower().split() if word not in stopwordlist] for doc in textlist]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "tweets = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "lst = []\n",
    "length = 0\n",
    "def get_topic(tweets):\n",
    "    topic = lda.get_document_topics(tweets)\n",
    "    for i in range(len(topic)):\n",
    "        for j in range(len(topic[i])):\n",
    "            lst.append(topic[i][j][1])\n",
    "        length = len(lst) - len(topic[i])\n",
    "        if i == 0:\n",
    "            print (\"第%i条twitter最有可能属于topic%i，属于该topic的概率为：%.3f\" % (i+1, topic[i][lst.index(max(lst))][0], max(lst)))\n",
    "        else:\n",
    "            print(\"第%i条twitter最有可能属于topic%i，属于该topic的概率为：%.3f\" % (i+1, topic[i][lst[length:].index(max(lst[length:]))][0], max(lst[length:])))\n",
    "\n",
    "get_topic(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tf = tfidf.fit_transform(BodyText)\n",
    "tf_feature_names = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnnie/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=20, n_jobs=1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=20)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: roger yeah counts sheets yup congratulate discouraging courier philippes inquiring pouch cin angolans bowl ssci stephens younis pronounciation bios retried\n",
      "Topic #1: ok gheit roccos iiloty donnas mccue meghann redraft trimming nasrallah breathlessness lyi serendipitous zaki hbi hossam screed arming thats referring\n",
      "Topic #2: pis print lock ns shelton prothero qs laurenpls amendmentcould thinking fine drafts ill comfortable thx email fw fax september day\n",
      "Topic #3: pm secretarys office arrive room meeting en depart route residence private department daily conference monica state kris floor andrews minutes\n",
      "Topic #4: thx benghazi doc supposed foia redactions waiver date select produced comm cb dept maggie perfect church fox secret thankfully data\n",
      "Topic #5: interesting aye haim mullens congratsiiiiii prelude betcha kinney leecia ophelia furstenberg binta kives hochberg rudin saban duke nasr clay whos\n",
      "Topic #6: yellow leg ffyi ba greet elmendorf morsis dmv license somewherein blumenthat hygienist mammal westerners gravy torso westerner leaned improves hoges\n",
      "Topic #7: release yep bb bbdb forwarded jakesullivar millscdstategov nss fedex aq viz tickets mailto backstory kamala ipad httpwwwbrookingsedumediafilesrcreportsarabopinionpolltelhamiarabopinionpollielhamipdf scan agreeable gene\n",
      "Topic #8: hoo whoo ubl rwwwwqs youi reax elabdin villiers katiestanton simons casper sawsanhassan eimanzein rene magariaf cretz privat hre pleaie lf\n",
      "Topic #9: joanne laszczych caitlin tps turner scrolling ydalo ijb holbrookellyj wher logo kdowd laurenjilo klevorick wchester concussion persorgb persorg hpsci susteren\n",
      "Topic #10: hows lucky cell vm hume dougs columns howe turningmight strauss beirne gertie momoima patsy reacherd halftime nephews shelly revision rosemarie\n",
      "Topic #11: calling printed shouldve fyl ihave devil fysa pasting ditechcom dkp pipher steinem pray buddhist chronicles copy mazen revcon tt boswell\n",
      "Topic #12: sounds good goh bosworth victoria lunching februa dni gloria bunkers delong sendng nuland hubbard bass ib menu janua pat coming\n",
      "Topic #13: yes importance high drafting farmer beats schwerin downstairs loretta rankers jakesulliva doable wwjc scenesetter offc reestablish turkeys messae tuff vous\n",
      "Topic #14: flagged newsweek dyi wolfe nb renegade bic repressed nwt enacts wrote fyi right adding addressing imprison flag draconian palmer myers\n",
      "Topic #15: compliment photo encouraging saud lebanon polls exit whos joining winning true aq thanked prince passing apologize israelis consider ubl surprise\n",
      "Topic #16: npr recipients diagnostic mswashdcibwashdcstatesbu microsoft vancestategov clntinetclintonlocal jun fvl kyl svtc ool mapi resending redeliver headers server smtp smtpsvc esmtp\n",
      "Topic #17: great christmas added merry cisco gathering schedules lippert touche harvey ibrahim diving olebmp patraeus drives magnificent ifyou gladly deleted heyman\n",
      "Topic #18: fyi pls print declassify reason imagejpg confidential droid wow messa maura original barnett bd clearance recieved amcit vet paymasters fro\n",
      "Topic #19: pm know just tomorrow talk im cheryl today fw huma state did time thx want good work think mills cdm\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda, tf_feature_names, n_top_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 395\n",
      "INFO:lda:vocab_size: 4258\n",
      "INFO:lda:n_words: 84010\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 1500\n",
      "INFO:lda:<0> log likelihood: -1051748\n",
      "INFO:lda:<10> log likelihood: -719800\n",
      "INFO:lda:<20> log likelihood: -699115\n",
      "INFO:lda:<30> log likelihood: -689370\n",
      "INFO:lda:<40> log likelihood: -684918\n",
      "INFO:lda:<50> log likelihood: -681322\n",
      "INFO:lda:<60> log likelihood: -678979\n",
      "INFO:lda:<70> log likelihood: -676598\n",
      "INFO:lda:<80> log likelihood: -675383\n",
      "INFO:lda:<90> log likelihood: -673316\n",
      "INFO:lda:<100> log likelihood: -672761\n",
      "INFO:lda:<110> log likelihood: -671320\n",
      "INFO:lda:<120> log likelihood: -669744\n",
      "INFO:lda:<130> log likelihood: -669292\n",
      "INFO:lda:<140> log likelihood: -667940\n",
      "INFO:lda:<150> log likelihood: -668038\n",
      "INFO:lda:<160> log likelihood: -667429\n",
      "INFO:lda:<170> log likelihood: -666475\n",
      "INFO:lda:<180> log likelihood: -665562\n",
      "INFO:lda:<190> log likelihood: -664920\n",
      "INFO:lda:<200> log likelihood: -664979\n",
      "INFO:lda:<210> log likelihood: -664722\n",
      "INFO:lda:<220> log likelihood: -664459\n",
      "INFO:lda:<230> log likelihood: -664360\n",
      "INFO:lda:<240> log likelihood: -663600\n",
      "INFO:lda:<250> log likelihood: -664164\n",
      "INFO:lda:<260> log likelihood: -663826\n",
      "INFO:lda:<270> log likelihood: -663458\n",
      "INFO:lda:<280> log likelihood: -663393\n",
      "INFO:lda:<290> log likelihood: -662904\n",
      "INFO:lda:<300> log likelihood: -662294\n",
      "INFO:lda:<310> log likelihood: -662031\n",
      "INFO:lda:<320> log likelihood: -662430\n",
      "INFO:lda:<330> log likelihood: -661601\n",
      "INFO:lda:<340> log likelihood: -662108\n",
      "INFO:lda:<350> log likelihood: -662152\n",
      "INFO:lda:<360> log likelihood: -661899\n",
      "INFO:lda:<370> log likelihood: -661012\n",
      "INFO:lda:<380> log likelihood: -661278\n",
      "INFO:lda:<390> log likelihood: -661085\n",
      "INFO:lda:<400> log likelihood: -660418\n",
      "INFO:lda:<410> log likelihood: -660510\n",
      "INFO:lda:<420> log likelihood: -660343\n",
      "INFO:lda:<430> log likelihood: -659789\n",
      "INFO:lda:<440> log likelihood: -659336\n",
      "INFO:lda:<450> log likelihood: -659039\n",
      "INFO:lda:<460> log likelihood: -659329\n",
      "INFO:lda:<470> log likelihood: -658707\n",
      "INFO:lda:<480> log likelihood: -658879\n",
      "INFO:lda:<490> log likelihood: -658819\n",
      "INFO:lda:<500> log likelihood: -658407\n",
      "INFO:lda:<510> log likelihood: -658651\n",
      "INFO:lda:<520> log likelihood: -658111\n",
      "INFO:lda:<530> log likelihood: -658018\n",
      "INFO:lda:<540> log likelihood: -658111\n",
      "INFO:lda:<550> log likelihood: -657925\n",
      "INFO:lda:<560> log likelihood: -657860\n",
      "INFO:lda:<570> log likelihood: -657494\n",
      "INFO:lda:<580> log likelihood: -657723\n",
      "INFO:lda:<590> log likelihood: -657591\n",
      "INFO:lda:<600> log likelihood: -657557\n",
      "INFO:lda:<610> log likelihood: -657505\n",
      "INFO:lda:<620> log likelihood: -657730\n",
      "INFO:lda:<630> log likelihood: -657304\n",
      "INFO:lda:<640> log likelihood: -657208\n",
      "INFO:lda:<650> log likelihood: -657518\n",
      "INFO:lda:<660> log likelihood: -657541\n",
      "INFO:lda:<670> log likelihood: -657381\n",
      "INFO:lda:<680> log likelihood: -657575\n",
      "INFO:lda:<690> log likelihood: -656985\n",
      "INFO:lda:<700> log likelihood: -656815\n",
      "INFO:lda:<710> log likelihood: -656930\n",
      "INFO:lda:<720> log likelihood: -656538\n",
      "INFO:lda:<730> log likelihood: -656291\n",
      "INFO:lda:<740> log likelihood: -656417\n",
      "INFO:lda:<750> log likelihood: -656747\n",
      "INFO:lda:<760> log likelihood: -656600\n",
      "INFO:lda:<770> log likelihood: -656269\n",
      "INFO:lda:<780> log likelihood: -656311\n",
      "INFO:lda:<790> log likelihood: -656069\n",
      "INFO:lda:<800> log likelihood: -656228\n",
      "INFO:lda:<810> log likelihood: -656178\n",
      "INFO:lda:<820> log likelihood: -655694\n",
      "INFO:lda:<830> log likelihood: -655997\n",
      "INFO:lda:<840> log likelihood: -656224\n",
      "INFO:lda:<850> log likelihood: -656197\n",
      "INFO:lda:<860> log likelihood: -655889\n",
      "INFO:lda:<870> log likelihood: -656180\n",
      "INFO:lda:<880> log likelihood: -656997\n",
      "INFO:lda:<890> log likelihood: -655989\n",
      "INFO:lda:<900> log likelihood: -655615\n",
      "INFO:lda:<910> log likelihood: -655584\n",
      "INFO:lda:<920> log likelihood: -656602\n",
      "INFO:lda:<930> log likelihood: -656083\n",
      "INFO:lda:<940> log likelihood: -656294\n",
      "INFO:lda:<950> log likelihood: -656257\n",
      "INFO:lda:<960> log likelihood: -656243\n",
      "INFO:lda:<970> log likelihood: -656028\n",
      "INFO:lda:<980> log likelihood: -655603\n",
      "INFO:lda:<990> log likelihood: -656012\n",
      "INFO:lda:<1000> log likelihood: -655849\n",
      "INFO:lda:<1010> log likelihood: -655376\n",
      "INFO:lda:<1020> log likelihood: -655417\n",
      "INFO:lda:<1030> log likelihood: -655856\n",
      "INFO:lda:<1040> log likelihood: -655197\n",
      "INFO:lda:<1050> log likelihood: -655938\n",
      "INFO:lda:<1060> log likelihood: -655529\n",
      "INFO:lda:<1070> log likelihood: -655092\n",
      "INFO:lda:<1080> log likelihood: -655119\n",
      "INFO:lda:<1090> log likelihood: -656215\n",
      "INFO:lda:<1100> log likelihood: -655602\n",
      "INFO:lda:<1110> log likelihood: -655296\n",
      "INFO:lda:<1120> log likelihood: -655547\n",
      "INFO:lda:<1130> log likelihood: -655580\n",
      "INFO:lda:<1140> log likelihood: -655604\n",
      "INFO:lda:<1150> log likelihood: -655168\n",
      "INFO:lda:<1160> log likelihood: -655281\n",
      "INFO:lda:<1170> log likelihood: -655409\n",
      "INFO:lda:<1180> log likelihood: -655517\n",
      "INFO:lda:<1190> log likelihood: -654922\n",
      "INFO:lda:<1200> log likelihood: -655304\n",
      "INFO:lda:<1210> log likelihood: -655852\n",
      "INFO:lda:<1220> log likelihood: -655184\n",
      "INFO:lda:<1230> log likelihood: -655650\n",
      "INFO:lda:<1240> log likelihood: -655606\n",
      "INFO:lda:<1250> log likelihood: -656086\n",
      "INFO:lda:<1260> log likelihood: -655698\n",
      "INFO:lda:<1270> log likelihood: -655351\n",
      "INFO:lda:<1280> log likelihood: -655686\n",
      "INFO:lda:<1290> log likelihood: -654801\n",
      "INFO:lda:<1300> log likelihood: -654973\n",
      "INFO:lda:<1310> log likelihood: -655186\n",
      "INFO:lda:<1320> log likelihood: -655128\n",
      "INFO:lda:<1330> log likelihood: -655365\n",
      "INFO:lda:<1340> log likelihood: -655338\n",
      "INFO:lda:<1350> log likelihood: -655219\n",
      "INFO:lda:<1360> log likelihood: -655115\n",
      "INFO:lda:<1370> log likelihood: -654930\n",
      "INFO:lda:<1380> log likelihood: -655209\n",
      "INFO:lda:<1390> log likelihood: -654940\n",
      "INFO:lda:<1400> log likelihood: -655055\n",
      "INFO:lda:<1410> log likelihood: -655286\n",
      "INFO:lda:<1420> log likelihood: -655316\n",
      "INFO:lda:<1430> log likelihood: -655257\n",
      "INFO:lda:<1440> log likelihood: -654964\n",
      "INFO:lda:<1450> log likelihood: -654884\n",
      "INFO:lda:<1460> log likelihood: -655493\n",
      "INFO:lda:<1470> log likelihood: -655415\n",
      "INFO:lda:<1480> log likelihood: -655192\n",
      "INFO:lda:<1490> log likelihood: -655728\n",
      "INFO:lda:<1499> log likelihood: -655858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: british churchill sale million major letters west britain\n",
      "Topic 1: church government political country state people party against\n",
      "Topic 2: elvis king fans presley life concert young death\n",
      "Topic 3: yeltsin russian russia president kremlin moscow michael operation\n",
      "Topic 4: pope vatican paul john surgery hospital pontiff rome\n",
      "Topic 5: family funeral police miami versace cunanan city service\n",
      "Topic 6: simpson former years court president wife south church\n",
      "Topic 7: order mother successor election nuns church nirmala head\n",
      "Topic 8: charles prince diana royal king queen parker bowles\n",
      "Topic 9: film french france against bardot paris poster animal\n",
      "Topic 10: germany german war nazi letter christian book jews\n",
      "Topic 11: east peace prize award timor quebec belo leader\n",
      "Topic 12: n't life show told very love television father\n",
      "Topic 13: years year time last church world people say\n",
      "Topic 14: mother teresa heart calcutta charity nun hospital missionaries\n",
      "Topic 15: city salonika capital buddhist cultural vietnam byzantine show\n",
      "Topic 16: music tour opera singer israel people film israeli\n",
      "Topic 17: church catholic bernardin cardinal bishop wright death cancer\n",
      "Topic 18: harriman clinton u.s ambassador paris president churchill france\n",
      "Topic 19: city museum art exhibition century million churches set\n",
      "0 UK: Prince Charles spearheads British royal revolution. LONDON 1996-08-20 (top topic: 8)\n",
      "1 GERMANY: Historic Dresden church rising from WW2 ashes. DRESDEN, Germany 1996-08-21 (top topic: 13)\n",
      "2 INDIA: Mother Teresa's condition said still unstable. CALCUTTA 1996-08-23 (top topic: 14)\n",
      "3 UK: Palace warns British weekly over Charles pictures. LONDON 1996-08-25 (top topic: 8)\n",
      "4 INDIA: Mother Teresa, slightly stronger, blesses nuns. CALCUTTA 1996-08-25 (top topic: 14)\n",
      "5 INDIA: Mother Teresa's condition unchanged, thousands pray. CALCUTTA 1996-08-25 (top topic: 14)\n",
      "6 INDIA: Mother Teresa shows signs of strength, blesses nuns. CALCUTTA 1996-08-26 (top topic: 14)\n",
      "7 INDIA: Mother Teresa's condition improves, many pray. CALCUTTA, India 1996-08-25 (top topic: 14)\n",
      "8 INDIA: Mother Teresa improves, nuns pray for \"miracle\". CALCUTTA 1996-08-26 (top topic: 14)\n",
      "9 UK: Charles under fire over prospect of Queen Camilla. LONDON 1996-08-26 (top topic: 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "X = lda.datasets.load_reuters()\n",
    "vocab = lda.datasets.load_reuters_vocab()\n",
    "titles = lda.datasets.load_reuters_titles()\n",
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "model.fit(X)  # model.fit_transform(X) is also available\n",
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "doc_topic = model.doc_topic_\n",
    "for i in range(10):\n",
    "    print(\"{} (top topic: {})\".format(titles[i], doc_topic[i].argmax()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
