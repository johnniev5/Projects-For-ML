{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本数据特征工程\n",
    "\n",
    "* BOW/词袋模型 CountVector\n",
    "* TF-IDF TfidfVectorizer\n",
    "* word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本分类模型\n",
    "\n",
    "* NB/SVM/GBDT\n",
    "* Fasttext\n",
    "* CNN/LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们试用朴素贝叶斯完成一个中文文本分类器，一般在数据量足够，数据丰富足够的情况下，用朴素贝叶斯完成这个任务，准确度还是很不错的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习的算法要取得好效果，离不开数据，先来加载数据..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先挑选科技、汽车、娱乐、军事、运动共5类文本数据进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_technology = pd.read_csv(\"./data/technology_news.csv\", encoding='utf-8')\n",
    "df_technology = df_technology.dropna()\n",
    "\n",
    "df_car = pd.read_csv(\"./data/car_news.csv\", encoding='utf-8')\n",
    "df_car = df_car.dropna()\n",
    "\n",
    "df_entertainment = pd.read_csv(\"./data/entertainment_news.csv\", encoding='utf-8')\n",
    "df_entertainment = df_entertainment.dropna()\n",
    "\n",
    "df_military = pd.read_csv(\"./data/military_news.csv\", encoding='utf-8')\n",
    "df_military = df_military.dropna()\n",
    "\n",
    "df_sports = pd.read_csv(\"./data/sports_news.csv\", encoding='utf-8')\n",
    "df_sports = df_sports.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "technology = df_technology.content.values.tolist()[1000:21000]\n",
    "technology = [x for x in [\" \".join(technology[i].split()) for i in range(len(technology))] if x]\n",
    "car = df_car.content.values.tolist()[1000:21000]\n",
    "car = [x for x in [\" \".join(car[i].split()) for i in range(len(car))] if x]\n",
    "entertainment = df_entertainment.content.values.tolist()[:20000]\n",
    "entertainment = [x for x in [\" \". join(entertainment[i].split()) for i in range(len(entertainment))] if x]\n",
    "military = df_military.content.values.tolist()[:20000]\n",
    "military = [x for x in [\" \".join(military[i].split()) for i in range(len(military))] if x]\n",
    "sports = df_sports.content.values.tolist()[:20000]\n",
    "sports = [x for x in [\" \".join(sports[i].split()) for i in range(len(sports))] if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'现在家里都拉了网线，都能无线上网，一定要帮他们先登上WiFi，另外，老人不懂得流量是什么，也不知道如何开关，控制流量，所以设置好流量上限很重要，免得不小心点开了视频或者下载，电话费就大发了。'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "technology[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'截至发稿时，人人车给出的处理方案仍旧是检修车辆。王先生则认为，车辆在购买时就存在问题，但交易平台并未能检测出来。因此，王先生希望对方退款。王先生称，他将找专业机构对车辆进行鉴定，并通过法律途径维护自己的权益。J256'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'央视记者 胡善敏：我现在所处的位置是在辽宁舰的飞行甲板，执行跨海区训练和试验任务的辽宁舰官兵，正在展开多个科目的训练，穿着不同颜色服装的官兵在紧张的对舰载机进行转运。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "military[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'据统计，2016年仅在中国田径协会注册的马拉松赛事便达到了328场，继续呈现出爆发式增长的态势，2015年，这个数字还仅仅停留在134场。如果算上未在中国田协注册的纯“民间”赛事，国内全年的路跑赛事还要更多。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词与中文文本处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中文停止词过滤与生成训练数据（降噪）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = pd.read_csv('./data/stopwords.txt', index_col=False, \n",
    "                       quoting=3, sep='\\t', names=['stopword'], encoding='utf-8')\n",
    "stopword = stopword.stopword.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['!', '\"', '#', ..., '07', '08', '09'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/dq/s5dx7lqd1nl0kf3829swj_3w0000gn/T/jieba.cache\n",
      "Loading model cost 1.123 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "def preprocess_text(content, sentences, category):\n",
    "        for line in content:\n",
    "            try:\n",
    "                segs = jieba.lcut(line)\n",
    "                segs = filter(lambda x: len(x) > 1, segs)\n",
    "                segs = filter(lambda x: x not in stopword, segs)\n",
    "                sentences.append((\" \".join(segs), category))\n",
    "            except Exception:\n",
    "                print(line)\n",
    "                continue\n",
    "            \n",
    "preprocess_text(technology, sentences, 'technology')    \n",
    "preprocess_text(car, sentences, 'car') \n",
    "preprocess_text(entertainment, sentences, 'entertainment')    \n",
    "preprocess_text(military, sentences, 'military')\n",
    "preprocess_text(sports, sentences, 'sports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成训练集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打乱顺序，生成更可靠的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本次 赛事 骑师 阵营 中将 五名 中国 优秀 骑师 中国 骑师 首次 征战 成都 迪拜 国际 五名 中国 骑师 出战 两场 骑师 挑战赛 外国 骑师 同场竞技 一较高下 刷新 中国 速度 赛马 历史 sports\n",
      "本次 峰会 联盟 功能 联合 各大 正品 汽车 电子 品牌 苏宁 易购 双线 渠道 用户 提供 货真价实 产品 联盟 产品 价格 品质 售后 做出 规格 确保 苏宁 商家 联合 作保 质量 服务 双翼 齐飞 car\n",
      "亮剑 行业 树立 规范 标准 technology\n",
      "迄今为止 通信 行业 5G 规范 定义 以往 通信 技术 更新 一代 周期 推算 5G 2021 商用 technology\n",
      "昨天下午 圆通 工作人员 媒体 回应 北京 圆通 并未 倒闭 网友 站点 正好 一辆 快递 卸车 积压 欠薪 倒闭 大面积 情况 属实 春节 期间 压货 网点 人手 导致 派件 延期 快递 延迟 派送 拨打 客服 电话 提供 投诉 投诉 成功 后会 快递 给予 一百元 处罚 technology\n",
      "考生 考生 家长 entertainment\n",
      "当天 影片 众生 预告片 首度 曝光 唐僧 师徒 西天 取经 一路 斩妖 降魔 吴亦凡 饰演 唐僧 为情所困 导致 团队 内部 裂痕 矛盾重重 更新 饰演 孙悟空 扬言 师父 恩断义绝 摘掉 头上 金箍 唐僧 entertainment\n",
      "中国 汽车 合资企业 走过 三十余年 资本 合作 技术 合作 品牌 技术 合作 深度 阶段 广汽 三菱 发布 全新 企业 标识 传达 出广汽 集团 三菱 汽车 三菱 商事 平等 互利 发展 态势 广汽 集团 三菱 汽车 紧密 合作 深化 沟通 合作 机制 短短 一个月 高层 交流 会晤 步入 合作 深化 阶段 car\n",
      "中国 电子商务 发展 南亚 东南亚 国家 方兴未艾 世界 各大 电商 巨头 看好 潜力 无限 东南亚 南亚 市场 technology\n",
      "每座 城市 年轻人 忙于 生活 初入 社会 满怀 憧憬 日子 趋于 平淡 想要 重拾 过往 轻松 步伐 难上加难 平淡 转为 有趣 知名 主持人 百克 秘诀 工作 空隙 城市 展开 有趣 探索 周五 常德路 澳门路 新疆 巴扎 土特产 新疆 老乡 上海 改良 新疆 菜馆 品尝 美食 小店 充当 军师 空闲 时间 来到 黄埔江 唯一 封闭式 内陆 复兴岛 封存 时光 岛上 走走 逛逛 感受 这座 城市 呼吸 脉络 car\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences[:10]:\n",
    "    print(sentence[0], sentence[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用sklearn分割训练集数据，得到部分验证集数据用于检测分类器效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = zip(*sentences)\n",
    "X, y = zip(*[(x, y) for x, y in zip(X, y) if x !=''])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于词袋模型的文本特征抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=20000, min_df=1,\n",
       "        ngram_range=(1, 4), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CountVec = CountVectorizer(\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 4),\n",
    "    max_features=20000)\n",
    "CountVec.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里可以通过调整ngram_range来扩大词向量的个数，也就是增大特征输入，默认是1_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(CountVec.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下准确率如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8777415053565681"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(CountVec.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更可靠的验证方式是交叉验证，但是交叉验证最好也需要保证样本类别的均衡性，在这里我们使用StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "scores = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "for train_index, test_index in skf.split(CountVec.transform(X), np.array(y)):\n",
    "    X_train, X_test = CountVec.transform(X)[train_index], CountVec.transform(X)[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train, y_train)\n",
    "    scores.append(mnb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8828066442099336"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义为Class类对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "class TextClassifier():\n",
    "    \n",
    "    def __init__(self, classifier=MultinomialNB()):\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,4), \n",
    "                                          max_features=20000)\n",
    "        \n",
    "    def features(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.classifier.predict(self.features([x]))\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(self.features(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = TextClassifier()\n",
    "text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['technology'], dtype='<U13')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier.predict('苹果 公司 有 新 的 发布 计划')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8777415053565681"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(CountVec.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8490045519334223"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(CountVec.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 写成Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier():\n",
    "    \n",
    "    def __init__(self, classifier=SVC(kernel='linear')):\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=12000)\n",
    "\n",
    "    def features(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.classifier.predict(self.features([x]))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(self.features(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = TextClassifier()\n",
    "text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['military'], dtype='<U13')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier.predict('这 是 有史以来 最 大 的 一 次 军舰 演习')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8814198353947308"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
